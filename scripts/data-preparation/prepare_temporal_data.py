#!/usr/bin/env python3 """ Script to prepare temporal data Part of the streaming visualization project """ import pandas as pd import numpy as np from pathlib import Path import json import warnings from datetime import datetime, timedelta warnings.filterwarnings('ignore') def prepare_temporal_data(): """ Prepares temporal data for time series analysis """ print("PREPARING TEMPORAL DATA") print("=" * 50) # Path configuration data_dir = Path("../../analyst") output_dir = Path("../../visualizations/temporal") output_dir.mkdir(parents=True, exist_ok=True) # Country to region mapping country_to_region = { 'US': 'North America', 'CN': 'Asia', 'JP': 'Asia', 'DE': 'Europe', 'GB': 'Europe', 'IN': 'Asia', 'BR': 'South America', 'CA': 'North America', 'AU': 'Oceania', 'KR': 'Asia' } # Crypto to content mapping crypto_to_content = { 'Bitcoin': 'Drama', 'Ethereum': 'Comedy', 'BNB': 'Action', 'Solana': 'Documentary', 'Tether': 'Romance' } try: # Load integrated data print("Loading integrated data...") integrated_path = data_dir / "integrated_data.csv" if not integrated_path.exists(): print(f"ERROR: Not found {integrated_path}") return False df_integrated = pd.read_csv(integrated_path) print(f"OK: Integrated data loaded: {len(df_integrated)} records") # Apply mappings df_integrated['content_category'] = df_integrated['crypto'].map(crypto_to_content) df_integrated['region'] = df_integrated['country_code'].map(country_to_region) # Clean data df_integrated = df_integrated.dropna(subset=['content_category', 'region', 'Date']) # Convert date df_integrated['Date'] = pd.to_datetime(df_integrated['Date']) # Create temporal data print(" Creando datos temporales...") # Daily data by category daily_category = df_integrated.groupby(['Date', 'content_category']).agg({ 'interest': ['mean', 'sum', 'count'], 'volatility_7d': 'mean', 'Close': 'mean' }).round(3) daily_category.columns = ['avg_interest', 'total_interest', 'observation_count', 'avg_volatility', 'avg_price'] daily_category = daily_category.reset_index() # Daily data by region daily_region = df_integrated.groupby(['Date', 'region']).agg({ 'interest': ['mean', 'sum', 'count'], 'volatility_7d': 'mean' }).round(3) daily_region.columns = ['avg_interest', 'total_interest', 'observation_count', 'avg_volatility'] daily_region = daily_region.reset_index() # Weekly data (promedio por semana) df_integrated['week'] = df_integrated['Date'].dt.isocalendar().week df_integrated['year'] = df_integrated['Date'].dt.year weekly_data = df_integrated.groupby(['year', 'week', 'content_category']).agg({ 'interest': ['mean', 'sum'], 'volatility_7d': 'mean', 'Close': 'mean' }).round(3) weekly_data.columns = ['avg_interest', 'total_interest', 'avg_volatility', 'avg_price'] weekly_data = weekly_data.reset_index() # Calculate temporal metrics print(" Calculando métricas temporales...") # Trends by category category_trends = {} for category in df_integrated['content_category'].unique(): cat_data = daily_category[daily_category['content_category'] == category] if len(cat_data) > 1: # Calcular tendencia (pendiente de regresión lineal simple) x = np.arange(len(cat_data)) y = cat_data['avg_interest'].values trend = np.polyfit(x, y, 1)[0] # Pendiente category_trends[category] = { 'trend': float(trend), 'start_value': float(y[0]), 'end_value': float(y[-1]), 'change_percent': float((y[-1] - y[0]) / y[0] * 100) if y[0] != 0 else 0 } # Save temporal data print(" Guardando datos temporales...") # Save daily data by category daily_category_output = output_dir / "daily_category_data.csv" daily_category.to_csv(daily_category_output, index=False) print(f"OK: Daily data by category guardados: {daily_category_output}") # Save daily data by region daily_region_output = output_dir / "daily_region_data.csv" daily_region.to_csv(daily_region_output, index=False) print(f"OK: Daily data by region guardados: {daily_region_output}") # Save weekly data weekly_output = output_dir / "weekly_data.csv" weekly_data.to_csv(weekly_output, index=False) print(f"OK: Weekly data guardados: {weekly_output}") # Create summary temporal temporal_summary = { "date_range": { "start": str(df_integrated['Date'].min()), "end": str(df_integrated['Date'].max()), "total_days": int((df_integrated['Date'].max() - df_integrated['Date'].min()).days) }, "category_trends": category_trends, "total_observations": len(df_integrated), "avg_daily_interest": float(daily_category['avg_interest'].mean()), "max_daily_interest": float(daily_category['avg_interest'].max()), "min_daily_interest": float(daily_category['avg_interest'].min()) } summary_output = output_dir / "temporal_summary.json" with open(summary_output, 'w') as f: json.dump(temporal_summary, f, indent=2) print(f"OK: Temporal summary guardado: {summary_output}") print("\n TEMPORAL PREPARATION SUMMARY:") print(f" • Date range: {temporal_summary['date_range']['start']} a {temporal_summary['date_range']['end']}") print(f" • Total de días: {temporal_summary['date_range']['total_days']}") print(f" • Total observations: {temporal_summary['total_observations']}") print(f" • Daily average interest: {temporal_summary['avg_daily_interest']:.2f}") print("\n TRENDS BY CATEGORY:") for category, trend_data in category_trends.items(): direction = "" if trend_data['trend'] > 0 else "📉" if trend_data['trend'] < 0 else "➡️" print(f" {direction} {category}: {trend_data['change_percent']:+.1f}% change") return True except Exception as e: print(f"ERROR: Error durante la preparación temporal: {e}") return False if __name__ == "__main__": success = prepare_temporal_data() if success: print("\nOK: TEMPORAL DATA PREPARATION COMPLETED") else: print("\nERROR: ERROR IN TEMPORAL DATA PREPARATION") 